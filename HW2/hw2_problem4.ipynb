{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2, Problem 4\n",
    "\n",
    "The goal of this problem is for you to try and classify whether or not an individual is likely to make more or less than 50K per year.  Carry out this task.  Try at least five general architectures, report precision, recall and f1 score on a test set.\n",
    "\n",
    "For each of the parts, report your preformance in terms not of just numbers but in terms of graphs. When you have training and validation data, please show the curves as the training progresses. You should know when you are overfitting or underfitting. Don't just report bare numbers. **You are free to add implmentation or markdown cells to make your notebook clearer!!**\n",
    "\n",
    "## Data:\n",
    "\n",
    "The following dataset was taken from the first dataset repository: http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "\n",
    "As the original task of the dataset lays out, \n",
    "Please note:\n",
    "* the continuous variable fnlwgt represents final weight, which is the number of units in the target population that the responding unit represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dealing with Missing Values\n",
    "\n",
    "What should you do about dealing with missing values - do you just drop those rows?  \n",
    "One of the most common problems we come accross in working with data \"in the wild\" is missing data. Often we will have observations (rows) that have only some of the needed attributes. Different rows will have different attributes missing. There are a number of strategies for dealing with the missing values. Clearly one could be dropping the column (attribute), or row (observation). Unfortuntely if you drop columns you may lose critical information that is helpful for classification and may be present in most (many) of the rows. You can drop rows but if many rows have at least one missing value, you may loose too much data. Do you try to impute (i. e. fill in) the missing data?  If so how?  \n",
    "\n",
    "Explain why you chose the strategy you did.\n",
    "\n",
    "*Hint - '?' denotes a missing value.*\n",
    "\n",
    "### Some possible strategies for dealing with missing data\n",
    "\n",
    "1. Whenever there is pleanty of data and very little missing data, you should consider dropping rows and/or columns. This may introduce some bias in the data but again, if the problem is limited to a very few rows or columns, it is easy in training to reproduce.\n",
    "\n",
    "2. Fill with fixed value using sklearn.impute.SimpleImputer.\n",
    "    a. 'constant' 0. Rarely a good idea but sometimes, if we can assume that when it is missing it is basically 0, this might be a good idea. For example a data may list number of house fires in a zip code and a missing value just means none.\n",
    "    b. 'mean' if the data is numeric, the mean is meaningfull.\n",
    "    c. 'median' may be more sensible if the data is integer or ordered. When the mean and median are very different it is important to understand what a \"typical\" example might mean. When considering \"income\", for example, a few large outliers will mess up the mean.\n",
    "    d. \"most_frequent' when you have categorical (nominal) labels, mean and median don't make any sense. Most probable label is what you need to use. This is also known as \"mode\".\n",
    "\n",
    "3. sklearn.impute.MissingIndicator: Sometimes the fact that a value is missing, is itself an important indicator. One can create a new feature/attribute that indicates a certain attribute is missing. If you later build a classifier by hand you can explicitly wieght each variable using the missing variable weights so that for that example (row) that attribute won't contribute to the classifier. In a deep neural network it is possible that the network can learn to do that automatically.\n",
    "\n",
    "4. One can use the sklearn.impute.KNNImputer which will look for rows to fill in the data.\n",
    "\n",
    "5. Fill with sklearn.impute.IterativeImputer scikit-learn provides a sophisticated imputation strategy. You can read up on this in the documentation, but it will fix on of the columns (attributes), and try to use the other features to predict similar to KNN but more sophisticated.\n",
    "\n",
    "6. Train a classifier: You can build your own classifier using machine learning. This is kind of a problem within a problem but if done correctly, it has the potential to be more accurate than a simpler method. Of course, if done badly it could be worse.\n",
    "\n",
    "7. Manually impute the missing values. You may know enough about the problem to build an ad-hoc way to fill in the missing values for each column in a way that makes the most sense. This almost always requires a great deal of domain expertise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code for filling in the data here. Please end by using the appropriate bandas method\n",
    "# to show the amount of missing data (which in the end should not be any since you dropped or filled in data)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"age\",\n",
    "    \"work_class\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"target\"\n",
    "]\n",
    "df = pd.read_csv(\"adult.data\", names=columns)\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df[column] = df[column].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train Test Validate Split\n",
    "\n",
    "Ideally you will split the data and use the train data filling in proceedure for the test data. Because this is expensive you can do experiments initially to see if this matters. Just keep carefully in mind what you will know and what you can't know during the test evaluation. Both sklearn and tensorflow provide facilities for train test split. Take your pick.\n",
    "\n",
    "At the end of this you should have a train, validate and test split. In the next part you are going to do preliminary testing of your model with your train+validation sets to get some idea of good canditates for hyperparameters. Later you will merge your training and validation set and resplit them up using cross validation to get better estimates for setting hyper-parameters\n",
    "\n",
    "**NOTE: It is very important that you record very carefully any parameters you have for filling in data in step 1. For example if you you build a \"fit\" using some training data, later you will need to use the this \"fit\" to transform the data, you can not re-fit on new data. In other words if your \"pipline\" in training takes the mean of the input to fill in the first column, you need to fill with exactly that number, when you get new data for testing. Don't take the mean of the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your solution for a train-test split in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build different 2 sklearn models and five different variations of Keras/tensorflow  models\n",
    "\n",
    "Two simple models should be baseline from sklearn. Try a logistic regression and a random forest to know what your dense neural network should be able to beat. Use the training and validation data from above (don't look at the testing data). Try varations on the number of notes in a layer, the number of layers. Also play with feature selection. You can try to eliminate featuers and see if your validation score goes up or down. See how the batch size effects things.\n",
    "\n",
    "### At the end of this section provide a report with figures on your conclusion on how these things effected preformance:\n",
    "\n",
    "* number of hidden layers\n",
    "* number of nodes per hidden layer aka matrix dimension\n",
    "* activation function\n",
    "* weight initialization\n",
    "* metrics for evaluation\n",
    "* batch size\n",
    "* number of epochs\n",
    "* optimizer\n",
    "* also carry out feature selection / dimensionality reduction\n",
    "* does the model do better or worse with dimensionality reduction?\n",
    "\n",
    "** Note: do not try regularization yet!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline results here with logisic regression and random forest\n",
    "\n",
    "# Set up your models here\n",
    "\n",
    "def model_one():\n",
    "    pass\n",
    "\n",
    "def model_two():\n",
    "    pass\n",
    "\n",
    "def model_three():\n",
    "    pass\n",
    "\n",
    "def model_four():\n",
    "    pass\n",
    "\n",
    "def model_five():\n",
    "    pass\n",
    "\n",
    "\n",
    "# Perform preliminary evaluations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary conclusions on your models\n",
    "\n",
    "Include some graphs and peformance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cross-validation\n",
    "We really should have used k-fold (eg. k=5) crossvalidation here, to not only evaluate our five keras/tensorflow models. See how your preliminary results change. Now that we have validation results with uncertainy (+- standard deviation), do your prior conclusion change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 inplement cross validation here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in your Part 4 Conclusion here \n",
    "\n",
    "Explain what you conclude on your models comparing preliminary results to those after cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Refining with Regularization\n",
    "\n",
    "We know that our biggest problem, if our models are flexibile enough, will be overfitting. Please try to regularize your best 2 models to see if you can improve their results. Look at these questions:\n",
    "\n",
    "* Try regularizing each of your two best models, does the generalizability increase?  of Decrease?  \n",
    "* Is one more sensitive than the other? Why might this happen and why?  \n",
    "* Please try this with all of your features and then with the reduced set of features.  \n",
    "* Report your precision, recall and f1 score on the train and validation sets (no cross validatio yet).  \n",
    "* Next carry out cross validation.  Does regularization reduce under or overfitting?   Why or why not?  \n",
    "\n",
    "** Hint: Try both L1 or L2 norm for regularization or dropout **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your code analysis for part 5 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in your part 5 conclusions here\n",
    "\n",
    "Explain what you conclude from your regularization analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Grid Search\n",
    "\n",
    "Please read this [article on using Grid Search CV with Keras](https://mlfromscratch.com/gridsearch-keras-sklearn). Be very very careful. Grid Search is very very slow. Given your above experiments you should have a pretty good idea of what your prameters should be before you run grid search. Run some small gridsearch runs (much smaller or more widely spaced) than you want to estimate time. Remember google colab will end after a limited amount of time so build slowly. Try to pin down the best parameters for:\n",
    "\n",
    "1. The number of layers (please don't go deeper than 10 hidden layers)\n",
    "2. The number of nodes per layer\n",
    "3. The type of regularization to use\n",
    "4. The type of weight initialization to use.\n",
    "5. The type of activation function.\n",
    "6. The metric to evaluate with, although logloss is standard, try using other metrics of accuracy.  \n",
    "\n",
    "You may even try multiple and averaging or taking the harmonic weight of multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your implementation here with graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put your grid search conclusion for part 6 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusion\n",
    "\n",
    "Conclude with a full report here on what we know now about this problem. How well it does verses baseline, what the best Keras archtecture is, what features should be used, how the data should be cleaned etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
