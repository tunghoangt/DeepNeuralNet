{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "hw3_p4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VQPLTqliJCxY",
        "XLt3e_QQJCxl",
        "WQdDk-rIJCxr"
      ],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBdlENlyJCvC",
        "colab_type": "text"
      },
      "source": [
        "In this assignment we will look at a typical image based machine learning task.\n",
        "\n",
        "## Image classification \n",
        "\n",
        "For this task the whole image is used to classify what's happening.\n",
        "\n",
        "For this specific task, we will be trying to classify COVID-19 using pneumonia x-rays.  Please note, the literature has mostly suggested CT scans are not an effective way of figuring out what type of disease you have.  This exercise is for academic purposes _only_.\n",
        "\n",
        "Steps:\n",
        "\n",
        "\n",
        "1. Download the pneumonia data.  \n",
        "\n",
        "You can find it here:\n",
        "\n",
        "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "move the folder to this directory and unzip it.  Please don't change any folder names or the below script will not work.  Also make sure the folder is in the same directory as this notebook!\n",
        "\n",
        "2. load the pneumonia data into a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zsxIw9C6M1K",
        "colab_type": "code",
        "outputId": "7b6b26a4-6129-4cb0-e7d8-2ee0bc3a8579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFy8S-5l7GbD",
        "colab_type": "code",
        "outputId": "223c993f-2ab2-4fd0-cdb8-da521fcf4976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Check RAM usage\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime â†’ \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OncVJnbuKxXy",
        "colab_type": "code",
        "outputId": "12f03b96-50f9-441e-81c3-5ed457d41aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "colab = True\n",
        "download_data = False\n",
        "\n",
        "if colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "\n",
        "  import os\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/Colab Notebooks/chest-xray-pneumonia/\"\n",
        "\n",
        "  #changing the working directory\n",
        "  %cd /content/drive/My Drive/Colab Notebooks/chest-xray-pneumonia/\n",
        "\n",
        "  from google.colab.patches import cv2_imshow # display image in colab\n",
        "\n",
        "if download_data:\n",
        "  # Dowload Kaggle dataset and unzip\n",
        "  !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "  !unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/chest-xray-pneumonia/'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egw9b1lYajL1",
        "colab_type": "code",
        "outputId": "62af8e58-a77b-4d70-f149-523f008dc426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Tung"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Tung\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xVhJf7TJCvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import skimage.transform\n",
        "import skimage.color\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "EPOCHS = 10 # Change this latter\n",
        "BS = 8\n",
        "\n",
        "\n",
        "def load_training_data():\n",
        "    paths = [\n",
        "        \"chest_xray/train/NORMAL/*\",\n",
        "        \"chest_xray/train/PNEUMONIA/*\"\n",
        "    ]\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    for path in paths:\n",
        "        for im_path in glob.glob(path):\n",
        "            if path == \"chest_xray/train/NORMAL/*\":\n",
        "                labels.append(\"NORMAL\")\n",
        "            if path == \"chest_xray/train/PNEUMONIA/*\":\n",
        "                labels.append(\"PNEUMONIA\")\n",
        "            image_paths.append(im_path)\n",
        "    return np.array(image_paths), np.array(labels)\n",
        "\n",
        "def load_testing_data():\n",
        "    paths = [\n",
        "        \"chest_xray/test/NORMAL/*\",\n",
        "        \"chest_xray/test/PNEUMONIA/*\"\n",
        "    ]\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    for path in paths:\n",
        "        for im_path in glob.glob(path):\n",
        "            if path == \"chest_xray/test/NORMAL/*\":\n",
        "                labels.append(\"NORMAL\")\n",
        "            if path == \"chest_xray/test/PNEUMONIA/*\":\n",
        "                labels.append(\"PNEUMONIA\")\n",
        "            image_paths.append(im_path)\n",
        "    return np.array(image_paths), np.array(labels)\n",
        "\n",
        "train_paths, train_labels = load_training_data()\n",
        "test_paths, test_labels = load_testing_data()\n",
        "\n",
        "\n",
        "\n",
        "# Change here for shorter time\n",
        "runtime_lim = True\n",
        "num_sample = 1000\n",
        "if runtime_lim:\n",
        "  import random\n",
        "  random.seed(23)\n",
        "\n",
        "  train_idx = random.choices(range(len(train_paths)), k=num_sample)\n",
        "  test_idx = random.choices(range(len(test_paths)), k=num_sample)\n",
        "\n",
        "  train_paths = train_paths[train_idx]\n",
        "  train_labels = train_labels[train_idx]\n",
        "\n",
        "  test_paths = test_paths[test_idx]\n",
        "  test_labels = test_labels[test_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_E5zJOLJCvP",
        "colab_type": "text"
      },
      "source": [
        "3. read the data into memory, I recommend open-cv for this:\n",
        "\n",
        "`python -m pip install opencv-python` \n",
        "\n",
        "if you don't already have it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1-6Fn9hJCvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_images(image_paths):\n",
        "    return [Image.open(p, mode='r') for p in image_paths]\n",
        "\n",
        "train_images = load_images(train_paths) \n",
        "test_images = load_images(test_paths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg2ksfaMJCva",
        "colab_type": "text"
      },
      "source": [
        "4. resize the images to a standard size - \n",
        "\n",
        "Note: it ought to be a box.  So the width and height should be the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgkayfkXJCvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16 use 224x224\n",
        "image_size = 224\n",
        "\n",
        "def resize_and_greyscale(im):\n",
        "  im = im.resize((image_size, image_size))\n",
        "  return im.convert('L')\n",
        "\n",
        "def resize_images(images):\n",
        "    return [resize_and_greyscale(img) for img in images] \n",
        "\n",
        "train_images = resize_images(train_images)\n",
        "test_images = resize_images(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnpZONOgJCvi",
        "colab_type": "text"
      },
      "source": [
        "5. Greyscale the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD_Kf_L5JCvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greyscale_images(images):\n",
        "  pass\n",
        "  # return [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in images]\n",
        "\n",
        "# Comments out becasue:\n",
        "# resize and greyscale to reduce memory and computation\n",
        "\n",
        "# train_images = greyscale_images(train_images)\n",
        "# test_images = greyscale_images(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFmJ-TgBJCvq",
        "colab_type": "text"
      },
      "source": [
        "6. prepare the data for training the model.\n",
        "\n",
        "For this you'll need to transform the test and train image objects into a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vf4b--pJCvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_rgb1(im):\n",
        "  try:\n",
        "    w, h = im.shape\n",
        "  except:\n",
        "    return im\n",
        "  ret = np.empty((w, h, 3), dtype=np.uint8)\n",
        "  ret[:, :, 0] = im\n",
        "  ret[:, :, 1] = im\n",
        "  ret[:, :, 2] = im\n",
        "  return ret\n",
        "\n",
        "def img_to_array(im):\n",
        "  ret = np.array(im)\n",
        "  ret = to_rgb1(ret)\n",
        "  \n",
        "  return ret\n",
        "\n",
        "def features_to_np_array(images):\n",
        "  return np.array([img_to_array(img) for img in images])\n",
        " \n",
        "    \n",
        "\n",
        "train_images = features_to_np_array(train_images)\n",
        "test_images = features_to_np_array(test_images)\n",
        "\n",
        "# # Rescale\n",
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0\n",
        "# cv2_imshow(test_images[1])\n",
        "\n",
        "# tmp = img_to_array(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrjUjHC2JCv0",
        "colab_type": "text"
      },
      "source": [
        "Next you'll need to do the same for the labels:\n",
        "\n",
        "Note: You'll need to apply the `to_categorical` function after transforming to a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-44JgCnJCv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "code_map = {\"NORMAL\": 0, \"PNEUMONIA\": 1, 0: \"NORMAL\", 1: \"PNEUMONIA\"}\n",
        "\n",
        "def labels_to_np_array(labels):\n",
        "    labels = [code_map[l] for l in labels]\n",
        "    np.asarray(labels)\n",
        "    return to_categorical(labels, num_classes=2)\n",
        "\n",
        "train_labels = labels_to_np_array(train_labels)\n",
        "test_labels = labels_to_np_array(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH-pbGs0JCv6",
        "colab_type": "text"
      },
      "source": [
        "7. Seperate into train and test with `train_test_split` from scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeuyCc-hJCv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train test split code goes here\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = np.concatenate((train_images, test_images), axis = 0)\n",
        "y = np.concatenate((train_labels, test_labels), axis = 0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=52)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNmXTvIDJCv_",
        "colab_type": "text"
      },
      "source": [
        "8. Make the last four layers of VGG16 with imagenet weights trainable and then retrain the model.\n",
        "\n",
        "To understand how to do this, please see the following tutorial:\n",
        "\n",
        "https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjSbS5KqJCwA",
        "colab_type": "code",
        "outputId": "5160a188-0f28-4531-d13a-aa54ccaffb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "# your model and training code goes here\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.applications import VGG16\n",
        "\n",
        "\n",
        "#Load the VGG model\n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Only allow the last 4 layers trainable\n",
        "for layer in vgg_conv.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add the vgg convolutional base model\n",
        "model.add(vgg_conv)\n",
        "\n",
        "# Add new layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compule model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 21,137,986\n",
            "Trainable params: 13,502,722\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJDu3NqjX5CL",
        "colab_type": "code",
        "outputId": "380563fe-9be4-4fba-df52-5aa8262fe45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Model Train\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=BS, epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9497 - acc: 0.6776\n",
            "Epoch 2/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 3/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 4/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 5/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 6/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 7/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 8/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 9/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n",
            "Epoch 10/10\n",
            "1340/1340 [==============================] - 24s 18ms/step - loss: 4.9466 - acc: 0.6784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb260280e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM5bKTZhJCwF",
        "colab_type": "text"
      },
      "source": [
        "8. Check your score with classification_report from scikit-learn\n",
        "\n",
        "Now that you've trained your model, call `model.predict` to get the predicted values for classification.  \n",
        "Then compare your predicted values with y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUxyN4FNJCwH",
        "colab_type": "code",
        "outputId": "a61973e1-794d-484c-903f-ddc39694c603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "def print_classification_report(y_test, y_pred):\n",
        "  y_pred = y_pred > 0.5\n",
        "  y_pred = np.argmax(y_pred, axis=1) \n",
        "  y_test = y_test > 0.5\n",
        "  y_test = np.argmax(y_test, axis=1) \n",
        "  print(classification_report(y_test, y_pred, target_names=[\"NORMAL\", \"PNEUMONIA\"]))\n",
        "\n",
        "# classification report code goes here.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "target_names = [\"NORMAL\", \"PNEUMONIA\"]\n",
        "print_classification_report(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.00      0.00      0.00       205\n",
            "   PNEUMONIA       0.69      1.00      0.82       455\n",
            "\n",
            "    accuracy                           0.69       660\n",
            "   macro avg       0.34      0.50      0.41       660\n",
            "weighted avg       0.48      0.69      0.56       660\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JXFP_BsJCwO",
        "colab_type": "text"
      },
      "source": [
        "9. Data augmentation\n",
        "\n",
        "Now that you have a classifier, let's see if data augmentation improves things!  \n",
        "\n",
        "You can use the `ImageDataGenerator` that comes with keras.  Here's how to import it:\n",
        "\n",
        "`from tensorflow.keras.preprocessing.image import ImageDataGenerator`\n",
        "\n",
        "Here's the documentation: https://keras.io/preprocessing/image/\n",
        "\n",
        "Here's an example of it getting used in the wild, in case you get stuck:\n",
        "\n",
        "https://www.pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmovChXjJCwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# augment your data here\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "augDataGen = ImageDataGenerator(\n",
        "      rotation_range=20,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Change these to according to RAM usage\n",
        "train_batchsize = 100\n",
        "val_batchsize = 10\n",
        "\n",
        "\n",
        "trainGen = augDataGen.flow(X_train, y_train, batch_size=BS)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1SBnWxJJCwU",
        "colab_type": "text"
      },
      "source": [
        "10. retrain your classifier\n",
        "\n",
        "Now that you have augmented training data, please retrain your classifier.  The code should basically be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny0KOd9iJCwU",
        "colab_type": "code",
        "outputId": "919b3779-1303-4b24-ba60-287c096f2644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Create the model\n",
        "aug_model = models.Sequential()\n",
        "\n",
        "# new training code goes here\n",
        "\n",
        "# Add the vgg convolutional base model\n",
        "aug_model.add(vgg_conv)\n",
        "\n",
        "# Add new layers\n",
        "aug_model.add(layers.Flatten())\n",
        "aug_model.add(layers.Dense(64, activation='relu'))\n",
        "aug_model.add(layers.Dropout(0.5))\n",
        "aug_model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compule model\n",
        "opt = \"adam\"\n",
        "aug_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "aug_model.fit_generator(trainGen, steps_per_epoch=len(X_train)/BS,\n",
        "                        validation_data=(X_test, y_test), validation_steps=len(X_test)/BS,\n",
        "                        epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "168/167 [==============================] - 33s 196ms/step - loss: 10.3786 - acc: 0.3246 - val_loss: 10.5733 - val_acc: 0.3106\n",
            "Epoch 2/10\n",
            "168/167 [==============================] - 32s 193ms/step - loss: 10.4244 - acc: 0.3216 - val_loss: 10.5733 - val_acc: 0.3106\n",
            "Epoch 3/10\n",
            "168/167 [==============================] - 32s 193ms/step - loss: 6.6939 - acc: 0.5657 - val_loss: 4.7638 - val_acc: 0.6894\n",
            "Epoch 4/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 5.5612 - acc: 0.6388 - val_loss: 4.7638 - val_acc: 0.6894\n",
            "Epoch 5/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 5.6986 - acc: 0.6284 - val_loss: 4.7638 - val_acc: 0.6894\n",
            "Epoch 6/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 5.7214 - acc: 0.6284 - val_loss: 4.7638 - val_acc: 0.6894\n",
            "Epoch 7/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 6.0761 - acc: 0.6052 - val_loss: 10.5733 - val_acc: 0.3106\n",
            "Epoch 8/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 10.4244 - acc: 0.3216 - val_loss: 10.5733 - val_acc: 0.3106\n",
            "Epoch 9/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 10.4015 - acc: 0.3231 - val_loss: 10.5733 - val_acc: 0.3106\n",
            "Epoch 10/10\n",
            "168/167 [==============================] - 33s 194ms/step - loss: 10.4472 - acc: 0.3209 - val_loss: 10.5733 - val_acc: 0.3106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb0eb4bbfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCfBORhzJCwa",
        "colab_type": "text"
      },
      "source": [
        "11. re-evaluate your classifier\n",
        "\n",
        "Now that you've augmented the data, please re-evaluate your classifer.  Use classification report like before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmhz2yWTJCwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classification report goes here\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faDyhGtVJCwf",
        "colab_type": "text"
      },
      "source": [
        "12. Evaluate the difference with data augmentation and without:\n",
        "\n",
        "Did things improve?  Did they stay the same?  Did they get worse?  Please try to come up with an explanation of why you got the results you did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaAFNCqJCwg",
        "colab_type": "text"
      },
      "source": [
        "### Explanation of results go here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSAUBskcYwza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "covid_model = aug_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mgqzPwaJCwh",
        "colab_type": "text"
      },
      "source": [
        "13. Getting COVID19 data\n",
        "\n",
        "Now that you have a trained classifier with pneumonia, we are going to use this with COVID data.  \n",
        "\n",
        "Clone this repo:\n",
        "\n",
        "https://github.com/ieee8023/covid-chestxray-dataset\n",
        "\n",
        "use the clone command: `git clone [REPO]`\n",
        "\n",
        "to get the data locally.  \n",
        "\n",
        "Make sure to run this command in the same folder as this jupyter notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H3d_siKJCwi",
        "colab_type": "text"
      },
      "source": [
        "14. Read the data into memory\n",
        "\n",
        "The set up for this data repository is a little different.  Please use the following code to read the data into memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aNlhG1xJCwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_covid19():\n",
        "    base = \"covid-chestxray-dataset/\"\n",
        "    metadata = pd.read_csv(base+\"metadata.csv\")\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    for index, row in metadata.iterrows():\n",
        "        labels.append(row[\"finding\"])\n",
        "        image_paths.append(base+\"images/\"+row[\"filename\"])\n",
        "    return labels, image_paths\n",
        "\n",
        "labels, covid_image_paths = get_covid19()\n",
        "\n",
        "# Remove files with .gz in name\n",
        "for idx, path in reversed(list(enumerate(covid_image_paths))):\n",
        "  if \".gz\" in path:\n",
        "    labels.pop(idx)\n",
        "    covid_image_paths.pop(idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riGmkNZpJCwo",
        "colab_type": "text"
      },
      "source": [
        "15. preprocess images\n",
        "\n",
        "you'll need to run the following functions on this data:\n",
        "\n",
        "1. load_images\n",
        "2. resize_images\n",
        "3. greyscale_images\n",
        "4. features_to_np_array\n",
        "5. labels_to_np_array\n",
        "\n",
        "Make sure to run each of those functions in order!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Q9pKgHFmTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label reduction code goes here\n",
        "## Strip out labels other than 'No Finding' and 'COVID-19' from the dataset\n",
        "## do this before load image\n",
        "\n",
        "lop = [\"No Finding\", \"COVID-19\"]\n",
        "\n",
        "for i, label in reversed(list(enumerate(labels))):\n",
        "  if label not in lop:\n",
        "    covid_image_paths.pop(i)\n",
        "    labels.pop(i)\n",
        "\n",
        "# convert to np array\n",
        "covid_image_paths = np.array(covid_image_paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "if runtime_lim:\n",
        "  covid_idx = random.choices(range(len(covid_image_paths)), k=num_sample)\n",
        "  \n",
        "  covid_image_paths = covid_image_paths[covid_idx]\n",
        "  labels = labels[covid_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GislvbJ-JCwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add your function calls to covid_image_paths here\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "  img = Image.open(img_path, mode='r')\n",
        "  img = img.convert('L')\n",
        "  img = img.resize((image_size, image_size))\n",
        "  img = img_to_array(img)\n",
        "\n",
        "  return img/255.0\n",
        "\n",
        "covid_image = np.array([preprocess_image(img_path) for img_path in covid_image_paths])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoT3AsbAJCwt",
        "colab_type": "text"
      },
      "source": [
        "16. Strip out labels other than 'No Finding' and 'COVID-19' from the dataset\n",
        "\n",
        "There are two straight forward ways to do this:\n",
        "\n",
        "1) use a for-loop and keep track of indices\n",
        "\n",
        "2) read labels and features into a dataframe and then filter to those two label types.  Your choice!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIJA0pSpJCwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label reduction code goes here\n",
        "\n",
        "# Do this before 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPfZoAblJCwz",
        "colab_type": "text"
      },
      "source": [
        "17. Predict on the new images\n",
        "\n",
        "Here you'll use the classifier you trained on just pneumonia/not pneumonia to try and classify COVID-19 and no finding.  You'll use the pneumonia/not pneumonia classifier as a featurizer to do this.\n",
        "\n",
        "Much of the code has been written, you'll just need to supply your trained classifier as input.\n",
        "\n",
        "Please predict the labels from the classifier.  Then run `classification_report` to see how well your classifier did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOKUuO95JCw0",
        "colab_type": "code",
        "outputId": "fbb40ada-9c9a-410b-a8a9-6dd8d771607d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#prediction code goes here\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import glob\n",
        "import code\n",
        "\n",
        "def extract_features_covid(model, width, height):\n",
        "    base = \"covid-chestxray-dataset/\"\n",
        "    metadata = pd.read_csv(base+\"metadata.csv\")\n",
        "    labels = []\n",
        "    feature_list = []\n",
        "    image_paths = []\n",
        "    for index, row in metadata[:20].iterrows():\n",
        "        if row[\"finding\"] == \"COVID-19\" and \".gz\" not in row[\"filename\"]:\n",
        "            labels.append(\"COVID\")\n",
        "\n",
        "            im_path = base+\"images/\"+row[\"filename\"]\n",
        "            image_paths.append(im_path)\n",
        "\n",
        "            img = np.expand_dims(preprocess_image(im_path), axis=0)\n",
        "   \n",
        "            features = model.predict(img)\n",
        "            features_np = np.array(features)\n",
        "            feature_list.append(features_np.flatten())\n",
        "\n",
        "    return np.array(feature_list), labels\n",
        "\n",
        "def extract_features_not_covid(model, width, height):\n",
        "    feature_list = []\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    paths = [\n",
        "        \"chest_xray/test/NORMAL/*\",\n",
        "        \"chest_xray/test/PNEUMONIA/*\",\n",
        "        \"chest_xray/train/NORMAL/*\",\n",
        "        \"chest_xray/train/PNEUMONIA/*\"\n",
        "        \n",
        "    ]\n",
        "    for path in paths:\n",
        "        for im_path in glob.glob(path)[:20]:\n",
        "            if path == \"chest_xray/train/NORMAL/*\":\n",
        "                labels.append(\"CLEAR TRAIN\")\n",
        "                image_paths.append(im_path)\n",
        "            if path == \"chest_xray/test/NORMAL/*\":\n",
        "                labels.append(\"CLEAR TEST\")\n",
        "                image_paths.append(im_path)\n",
        "            if path == \"chest_xray/train/PNEUMONIA/*\":\n",
        "                labels.append(\"PNEUMONIA\")\n",
        "                image_paths.append(im_path)\n",
        "            # im = cv2.imread(im_path)\n",
        "            # im = cv2.resize(im, (width, height))\n",
        "      \n",
        "      \n",
        "            img = np.expand_dims(preprocess_image(im_path), axis=0)\n",
        "            features = model.predict(img)\n",
        "            features_np = np.array(features)\n",
        "            feature_list.append(features_np.flatten())\n",
        "\n",
        "    return np.array(feature_list), labels\n",
        "\n",
        "# please make a copy of your tuned model and save it to variable:\n",
        "untuned_model = covid_model\n",
        "\n",
        "# please specify the width and height you used for the image preprocessing\n",
        "width = image_size\n",
        "height = image_size\n",
        "\n",
        "covid_features, covid_labels = extract_features_covid(untuned_model, width, height)\n",
        "non_covid_features, non_covid_labels = extract_features_not_covid(untuned_model, width, height)\n",
        "features = np.concatenate([non_covid_features, covid_features])\n",
        "labels = covid_labels + non_covid_labels\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "for index, label in enumerate(labels):\n",
        "    if label == \"CLEAR TRAIN\":\n",
        "        X_train.append(features[index])\n",
        "        y_train.append(0)\n",
        "    if label == \"PNEUMONIA\":\n",
        "        X_train.append(features[index])\n",
        "        y_train.append(1)\n",
        "    if label == \"COVID\":\n",
        "        X_test.append(features[index])\n",
        "        y_test.append(1)\n",
        "    if label == \"CLEAR TEST\":\n",
        "        X_test.append(features[index])\n",
        "        y_test.append(0)\n",
        "\n",
        "logit_clf = LogisticRegression()\n",
        "logit_clf.fit(X_train, y_train)\n",
        "y_pred = logit_clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      1.00      0.83        20\n",
            "           1       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.71        28\n",
            "   macro avg       0.36      0.50      0.42        28\n",
            "weighted avg       0.51      0.71      0.60        28\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIKuf7Q5JCw3",
        "colab_type": "text"
      },
      "source": [
        "18. Compare and contrast how the classifier did on Pneumonia versus COVID-19\n",
        "\n",
        "Did it do as well?  Worse?  About the same?  What conclusions can you draw?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHGGD1kNJCw4",
        "colab_type": "text"
      },
      "source": [
        "### Add your answers here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY3lWgkOJCw7",
        "colab_type": "text"
      },
      "source": [
        "Now that we've looked at a bunch of base cases, let's see if we can improve things by changing the model architecture.  We'll do this with a bunch of discrete steps\n",
        "\n",
        "1. Change the number of trainable layers\n",
        "\n",
        "Here you will make more of the layers trainable.  For this we are going to use cross validation to try and figure out which the optimal number of trainable layers.  Please us from the last 6 layers to one layer.  So your range should be:\n",
        "\n",
        "```\n",
        "trainable_range = [-6, -5, -4, -3, -2, -1]\n",
        "```\n",
        "\n",
        "Also, your X and y data should be the pneumonia data only.  Since that's what we trained on.  We should not assume we have access to the COVID data, except for testing, which will do later on.\n",
        "\n",
        "Here's a blog post detailing how to set this up: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
        "\n",
        "Note you'll need to set the number of trainable layers inside of `model_create` in order to make this tunable.  \n",
        "\n",
        "Please report mean and standard deviation for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO2-JMnFsWBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=52)\n",
        "\n",
        "def new_model(activation='relu', optimizer='adam', dropout_rate=0.5, trainable_range=-4, num_nodes=64):\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  #Load the VGG model\n",
        "  vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "  # Only allow the last x layers trainable\n",
        "  for layer in vgg_conv.layers[:trainable_range]:\n",
        "      layer.trainable = False\n",
        "\n",
        "  # Create the model\n",
        "  model = models.Sequential()\n",
        "\n",
        "  # Add the vgg convolutional base model\n",
        "  model.add(vgg_conv)\n",
        "\n",
        "  # Add new layers\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(num_nodes, activation=activation))\n",
        "  model.add(layers.Dropout(dropout_rate))\n",
        "  model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "  # Compule model\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer=optimizer,\n",
        "                metrics=['acc'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbssloV2JCw9",
        "colab_type": "code",
        "outputId": "34dd6193-db1b-471b-ab39-96c712fe967c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "seed = 34\n",
        "np.random.seed(seed)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "grid_model = KerasClassifier(build_fn=new_model, batch_size=BS, epochs=EPOCHS, verbose=0)\n",
        "\n",
        "trainable_range = [-6, -4, -2]\n",
        "param_grid = dict(trainable_range=trainable_range)\n",
        "\n",
        "grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, n_jobs=1, verbose=0)\n",
        "\n",
        "grid_res = grid.fit(X_train[:5], y_train[:5])\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_res.best_score_, grid_res.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.600000 using {'trainable_range': -4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3oLLjuQNBdl",
        "colab_type": "code",
        "outputId": "acd14d07-c73f-4b9f-9cca-ff6af925629c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Cross validation code goes here\n",
        "\n",
        "best_trainable_range = grid_res.best_params_['trainable_range']\n",
        "\n",
        "scores_lst=[]\n",
        "\n",
        "y_train_label = np.argmax(y_train, axis=1)\n",
        "\n",
        "for train, test in kfold.split(X_train, y_train_label):\n",
        "  model = new_model(trainable_range = best_trainable_range)\n",
        "  model.fit(X_train[train], y_train[train], batch_size=BS, epochs=EPOCHS)\n",
        "  scores = model.evaluate(X_train[test], y_train[test])\n",
        "  scores_lst.append(scores)\n",
        "\n",
        "print(scores_lst)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.3951 - acc: 0.8540\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.2898 - acc: 0.8750\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.2926 - acc: 0.8708\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 3.6210 - acc: 0.7043\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9925 - acc: 0.6754\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9925 - acc: 0.6754\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9925 - acc: 0.6754\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9925 - acc: 0.6754\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9925 - acc: 0.6754\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9853 - acc: 0.6758\n",
            "536/536 [==============================] - 2s 4ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 3.4690 - acc: 0.6702\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.5129 - acc: 0.7729\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.3248 - acc: 0.8890\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 3.7091 - acc: 0.7379\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 5.0068 - acc: 0.6744\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9925 - acc: 0.6754\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9879 - acc: 0.6744\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 4.9996 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.7009 - acc: 0.6674\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6301 - acc: 0.6749\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6337 - acc: 0.6749\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6299 - acc: 0.6749\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6317 - acc: 0.6749\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6309 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6328 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6324 - acc: 0.6749\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6320 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6306 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "[[5.007446587975346, 0.6735074520111084], [4.9788325437858925, 0.6753731369972229], [4.978832636306535, 0.6753731369972229], [4.978832643423507, 0.6753731369972229], [0.6303083834363453, 0.6753731369972229]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzp5oINZJCw_",
        "colab_type": "text"
      },
      "source": [
        "2. Analyze your results\n",
        "\n",
        "Do you think that changing the number of tunable layers matters?  Does it improve classification accuracy enough to warrant changing the number of tunable layers?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-56ms0qRJCxA",
        "colab_type": "text"
      },
      "source": [
        "### Analysis and explanation go here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jEmG3DUJCxB",
        "colab_type": "text"
      },
      "source": [
        "2. Tune over a layer activation function\n",
        "\n",
        "Please set the number of tunable layers to 4 again.\n",
        "\n",
        "Now we are going to make the layer activation tunable.  \n",
        "\n",
        "To do this, please change the model_create function so that each layer has it's own tunable activation function.  Then run your new cross validation code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGyLNZRw9gHc",
        "colab_type": "code",
        "outputId": "acb6769e-d93a-4374-d504-55b8ada24304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "\n",
        "grid_model = KerasClassifier(build_fn=new_model, batch_size=BS, epochs=EPOCHS, verbose=0)\n",
        "\n",
        "activation = ['relu', 'tanh', 'sigmoid', 'linear']\n",
        "param_grid = dict(activation=activation)\n",
        "\n",
        "grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, n_jobs=1, verbose=0)\n",
        "\n",
        "grid_res = grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_res.best_score_, grid_res.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.719030 using {'activation': 'sigmoid'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0NbLL-rJCxC",
        "colab_type": "code",
        "outputId": "30e730ee-f824-43dd-f8f8-c0bdfd61bbf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# cross validation code goes here\n",
        "\n",
        "best_activation = grid_res.best_params_['activation']\n",
        "\n",
        "scores=[]\n",
        "\n",
        "y_train_label = np.argmax(y_train, axis=1)\n",
        "\n",
        "for train, test in kfold.split(X_train, y_train_label):\n",
        "  model = new_model(activation = best_activation)\n",
        "  model.fit(X_train[train], y_train[train], batch_size=BS, epochs=EPOCHS)\n",
        "  scores = model.evaluate(X_train[test], y_train[test])\n",
        "  scores.append(scores)\n",
        "\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.7535 - acc: 0.6590\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6948 - acc: 0.6115\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6622 - acc: 0.6507\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6492 - acc: 0.6632\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6402 - acc: 0.6693\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6386 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6351 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6389 - acc: 0.6754\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6354 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6348 - acc: 0.6754\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.7460 - acc: 0.5993\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6912 - acc: 0.6180\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6505 - acc: 0.6516\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6503 - acc: 0.6665\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6373 - acc: 0.6726\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6373 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6360 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6353 - acc: 0.6749\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6363 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6319 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.7706 - acc: 0.6059\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.7046 - acc: 0.6129\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6719 - acc: 0.6325\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6482 - acc: 0.6637\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6368 - acc: 0.6754\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6377 - acc: 0.6754\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6345 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6359 - acc: 0.6749\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6333 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6349 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.5305 - acc: 0.7309\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6433 - acc: 0.6628\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6524 - acc: 0.6600\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6478 - acc: 0.6698\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6413 - acc: 0.6726\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6390 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6363 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6356 - acc: 0.6749\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6378 - acc: 0.6744\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6376 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "Epoch 1/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.7463 - acc: 0.6026\n",
            "Epoch 2/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6923 - acc: 0.6325\n",
            "Epoch 3/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6691 - acc: 0.6474\n",
            "Epoch 4/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6450 - acc: 0.6665\n",
            "Epoch 5/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6437 - acc: 0.6744\n",
            "Epoch 6/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6381 - acc: 0.6749\n",
            "Epoch 7/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6385 - acc: 0.6749\n",
            "Epoch 8/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6373 - acc: 0.6744\n",
            "Epoch 9/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6361 - acc: 0.6749\n",
            "Epoch 10/10\n",
            "2144/2144 [==============================] - 11s 5ms/step - loss: 0.6370 - acc: 0.6749\n",
            "536/536 [==============================] - 2s 3ms/step\n",
            "[0.6305241887249163, 0.6753731369972229, [...]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNR8cy8JCxG",
        "colab_type": "text"
      },
      "source": [
        "3. Analyze your results\n",
        "\n",
        "Does your choice of activation function matter?  When does the activation function perform best?  \n",
        "\n",
        "Things to consider:\n",
        "\n",
        "* Specifically does choosing the same activation function for all of the layers do best? \n",
        "* Does choosing different activation functions for each of the layers do best?\n",
        "* Are they all within the same approximate accuracy range?\n",
        "* do things vary wildly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLdYPy9ZJCxI",
        "colab_type": "text"
      },
      "source": [
        "### Analysis and explanation go here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usx_QJq8JCxI",
        "colab_type": "text"
      },
      "source": [
        "4. Tune over more hyperparameters\n",
        "\n",
        "Now that we've tuned the activation functions, let's try tuning more parameters.  This time add tuning for the following parameters:\n",
        "\n",
        "* number of neurons per layer\n",
        "* weight initialization\n",
        "* optimizer\n",
        "* weight constraint\n",
        "* activation function\n",
        "* learning rate\n",
        "\n",
        "Here is a great post on the range of values you should consider: https://www.wandb.com/articles/fundamentals-of-neural-networks\n",
        "\n",
        "Here is some code that is also useful: https://www.kaggle.com/lavanyashukla01/training-a-neural-network-start-here\n",
        "\n",
        "for understanding this practically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOLs_kSwLdjE",
        "colab_type": "code",
        "outputId": "99f1a36f-d08a-48fd-f2fe-25ffcd6012ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "activation='relu', \n",
        "optimizer= ['SGD', 'adam']\n",
        "dropout_rates=[0.0, 0.2, 0.4, 0.6]\n",
        "num_nodes=[16, 32, 64, 128]\n",
        "\n",
        "\n",
        "param_grid = dict(activation=activation\n",
        "                  optimizer=optimizer,\n",
        "                  dropout_rate=dropout_rates,\n",
        "                  num_nodes=num_nodes)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_res = grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_res.best_score_, grid_res.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-2dcec8bc5420>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    optimizer=optimizer,\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikRTm2BfJCxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross validation code goes here\n",
        "\n",
        "# cross validation code goes here\n",
        "\n",
        "best_activation = grid_res.best_params_['activation']\n",
        "best_optimizer = grid_res.best_params_['optimizer']\n",
        "best_dropout_rates = grid_res.best_params_['dropout_rates']\n",
        "best_num_nodes = grid_res.best_params_['num_nodes']\n",
        "\n",
        "scores=[]\n",
        "\n",
        "y_train_label = np.argmax(y_train, axis=1)\n",
        "\n",
        "for train, test in kfold.split(X_train, y_train_label):\n",
        "  model = new_model(activation = best_activation, optimizer=best_optimizer, \n",
        "                    dropout_rates = best_dropout_rates, num_nodes = best_num_nodes)\n",
        "  \n",
        "  model.fit(X_train[train], y_train[train], batch_size=BS, epochs=EPOCHS)\n",
        "  scores = model.evaluate(X_train[test], y_train[test])\n",
        "  scores.append(scores)\n",
        "\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLqla_osJCxO",
        "colab_type": "text"
      },
      "source": [
        "5. Tune over data augmentation\n",
        "\n",
        "Here you'll take the best hyperparameters from your neural network, with 4 trainable layers, and then add them to a pipeline.  We will then tune over data augmentation parameters.  Report out your mean and standard deviation of accuracy.\n",
        "\n",
        "Here we will create a scikit-learn pipline:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
        "\n",
        "If you need an example with gridsearch and pipeline:\n",
        "\n",
        "https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
        "\n",
        "As a reminder, here is the documentation for data augmentation:\n",
        "\n",
        "https://keras.io/preprocessing/image/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cz6gXuFJCxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cross validation code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZHP2GY1JCxY",
        "colab_type": "text"
      },
      "source": [
        "6. Analyze your results\n",
        "\n",
        "Now that you've tuned over model parameters and preprocessing, what has a bigger impact?  Why do you think that might be the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQPLTqliJCxY",
        "colab_type": "text"
      },
      "source": [
        "### Analysis and explanation goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_IZpYy6JCxY",
        "colab_type": "text"
      },
      "source": [
        "7. Using your best model and preprocessing to train a new model\n",
        "\n",
        "Now you should select the best hyperparameters for the neural network and the best hyperparameters for the preprocesser and then combine them into a scikit-learn pipeline.  Next train a classifier with these new tuned hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17_FGCUvJCxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#classifer generation code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGk56ZKLJCxe",
        "colab_type": "text"
      },
      "source": [
        "8. Let's see if things improved - time for `classification_report`\n",
        "\n",
        "Now that you've tuned your model, let's see how well it does on our test set!  First call predict on the test data to get a prediction.  Then use `classification_report` to see how well the model does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAzXNcYZJCxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction code goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPLScFCUJCxl",
        "colab_type": "text"
      },
      "source": [
        "9. Analsis and comparison\n",
        "\n",
        "Now that you've seen how well your classifier does when it's been tuned, compare this with your previous model, that was untuned.  Are the precision, recall and f1-scores substantially different?  Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLt3e_QQJCxl",
        "colab_type": "text"
      },
      "source": [
        "### Analysis and explanation goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIDi1RwJCxm",
        "colab_type": "text"
      },
      "source": [
        "10. Prediction on COVID binary classification task with tuned model\n",
        "\n",
        "Now you'll use your tuned classifier to try and predict on the binary COVID19 case.  Please change the model to your tuned model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOArsDs-JCxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction code goes here\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import glob\n",
        "import code\n",
        "\n",
        "def extract_features_covid(model, width, height):\n",
        "    base = \"covid-chestxray-dataset/\"\n",
        "    metadata = pd.read_csv(base+\"metadata.csv\")\n",
        "    labels = []\n",
        "    image_paths = []\n",
        "    for index, row in metadata.iterrows():\n",
        "        if row[\"finding\"] == \"COVID-19\":\n",
        "            labels.append(\"COVID\")\n",
        "            image_paths.append(base+row[\"filename\"])\n",
        "            im = cv2.imread(im_path)\n",
        "            im = cv2.resize(im, (width, height))\n",
        "            features = model.predict(img)\n",
        "            features_np = np.array(features)\n",
        "            feature_list.append(features_np.flatten())\n",
        "\n",
        "    return np.array(feature_list), labels\n",
        "\n",
        "def extract_features_not_covid(model, width, height):\n",
        "    feature_list = []\n",
        "    labels = []\n",
        "    paths = [\n",
        "        \"chest_xray/test/NORMAL/*\",\n",
        "        \"chest_xray/test/PNEUMONIA/*\",\n",
        "        \"chest_xray/train/NORMAL/*\",\n",
        "        \"chest_xray/train/PNEUMONIA/*\"\n",
        "        \n",
        "    ]\n",
        "    for path in paths:\n",
        "        for im_path in glob.glob(path):\n",
        "            if path == \"chest_xray/train/NORMAL/*\":\n",
        "                labels.append(\"CLEAR TRAIN\")\n",
        "            if path == \"chest_xray/test/NORMAL/*\":\n",
        "                labels.append(\"CLEAR TEST\")\n",
        "            if path == \"chest_xray/train/PNEUMONIA/*\":\n",
        "                labels.append(\"PNEUMONIA\")\n",
        "            im = cv2.imread(im_path)\n",
        "            im = cv2.resize(im, (width, height))\n",
        "            features = model.predict(img)\n",
        "            features_np = np.array(features)\n",
        "            feature_list.append(features_np.flatten())\n",
        "\n",
        "    return np.array(feature_list), labels\n",
        "\n",
        "# please make a copy of your tuned model and save it to variable:\n",
        "# tuned_model = [YOUR MODEL NAME GOES HERE]\n",
        "\n",
        "# please specify the width and height you used for the image preprocessing\n",
        "# width = [YOUR WIDTH GOES HERE]\n",
        "# height = [YOUR HEIGHT GOES HERE]\n",
        "\n",
        "covid_features, covid_labels = extract_features_covid(tuned_model, width, height)\n",
        "non_covid_features, non_covid_labels = extract_features_not_covid(tuned_model, width, height)\n",
        "features = covid_features + non_covid_features\n",
        "labels = covid_labels + non_covid_labels\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "for index, label in enumerate(labels):\n",
        "    if label == \"CLEAR TRAIN\":\n",
        "        X_train.append(features[index])\n",
        "        y_train.append(0)\n",
        "    if label == \"PNEUMONIA\":\n",
        "        X_train.append(features[index])\n",
        "        y_train.append(1)\n",
        "    if label == \"COVID\":\n",
        "        X_test.append(features[index])\n",
        "        y_test.append(1)\n",
        "    if label == \"CLEAR TEST\":\n",
        "        X_test.append(features[index])\n",
        "        y_test.append(0)\n",
        "\n",
        "logit_clf = LogisticRegression()\n",
        "logit_clf.fit(X_train, y_train)\n",
        "y_pred = logit_clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GyYC2wJCxq",
        "colab_type": "text"
      },
      "source": [
        "11. Analyze your results\n",
        "\n",
        "Now that you've seen the results of your tuned model, compare those with the results of the untuned model.  Did things get better? Worse?  Why do you think this may or may not be the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQdDk-rIJCxr",
        "colab_type": "text"
      },
      "source": [
        "### Analysis of your results goes here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvLniiaMJCxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}